\documentclass[12pt,letterpaper]{article}

\usepackage{amsmath, amsthm}
\usepackage{graphicx,hyperref}
\usepackage{microtype, parskip}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{lineno}
\usepackage[font=small]{caption}
\usepackage{subcaption, multirow, morefloats}
\usepackage{subcaption, wrapfig}
\usepackage{titlesec}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{authblk, attrib, fullpage}
\usepackage{lineno}

\frenchspacing

\captionsetup[subfigure]{position = top, labelfont = bf, textfont = normalfont, singlelinecheck = off, justification = raggedright}

\renewcommand{\section}[1]{%
\bigskip
\begin{center}
\begin{Large}
\normalfont\scshape #1
\medskip
\end{Large}
\end{center}}

\renewcommand{\subsection}[1]{%
\bigskip
\begin{center}
\begin{large}
\normalfont\itshape #1
\end{large}
\end{center}}

\renewcommand{\subsubsection}[1]{%
\vspace{2ex}
\noindent
\textit{#1.}---}

\renewcommand{\tableofcontents}{}

\bibpunct{(}{)}{;}{a}{}{,}  % this is a citation format command for natbib

\title{How cryptic is cryptic diversity? Machine learning approaches to classifying morphological variation in \textit{Emys marmorata} (Testudinoidea, Emydidae).}
\author[1]{Peter D Smits}%\thanks{psmits@uchicago.edu}}
\author[1,2]{Kenneth D Angielczyk}%\thanks{kangielczyk@fieldmuseum.org}}
\author[3]{James F Parham}%\thanks{jparham@fullerton.edu}}
\affil[1]{Committee on Evolutionary Biology, University of Chicago}
\affil[2]{Integrative Research Center, Field Museum of Natural History}
\affil[3]{Department of Geological Sciences, California State University -- Fullerton}


\begin{document}
\maketitle
\noindent{\textbf{Corresponding author:} Peter D Smits, Committee on Evolutionary Biology, University of Chicago, 1025 E. 57th Street, Culver Hall 402, Chicago, IL, 60637, USA; E-mail: \href{mailto:psmits@uchicago.edu}{psmits@uchicago.edu}}

\linenumbers
\modulolinenumbers[2]

\begin{abstract}
\end{abstract}

\section{Introduction}

% cryptic diversity
%   most species are still deliminated solely based on morphology
With the rise of molecular systematics and techniques such as DNA barcoding, it has been increasingly recognized that some taxa can only be differentiated using molecular data \citep{Bickford2007,SchlickSteiner2007,Stuart2006,Pfenninger2007,Funk2012,Clare2011}. Nevertheless, the majority of extant taxa, and nearly all extinct taxa, are delimited solely via morphology. This disjunct is of great concern for studies of variation and diversity dynamics in deep time, where apparent morphological stasis may not reflect the true underlying diversity that existed \citep{Hunt2008,Eldredge1972,Gould1977a,VanBocxlaer2013}. 

Much work has been devoted to species delimitation via sequence difference \citep{Fujita2012,Yang2010b} which in turn has sparked interest in whether geometric morphometric anlaysis can capture similarly fine-scale variation that can be used for identifying cryptic species. Most such studies focus on morphometrics to discover differences between taxa that were identified by other means \citep{Polly2007a,Demandt2009,Gaubert2005b,Gunduz2007,Polly2003,Zelditch2004} and automated taxon identification \citep{MacLeod2007}. It would be very useful, though, if morphometrics approaches could reliably identify cryptic species without the use of other types of data. This would make the task of identifying and maintaining endangered or conserved groups much easier and could contribute to improved classifications of extinct taxa.

Here, we attempt to address this issue using machine learning approaches. In particular, we ask whether it is possible to determine which amongst a set of classification hypotheses is best and examine the implications of the results for a recently proposed set of cryptic turtle species.

% past work on automatic taxon identification and older approaches to classifying taxa
% why use machine learning methods
\subsection{Background and system}
Machine learning is in many respects just an extension of known statistical methodology \citep{Hastie2009} in which the emphasis is placed on high predictive accurate at the expense of interpretability of individual parameters. The basic statistical mechanics are supplemented by randomization, sorting, and partitioning algorithms and along with the maximization or minimization of summary statistics, in order to best estimate a general model for all data, both sampled and unsampled \citep{Hastie2009}. Machine learning approaches have found use in medical research, epidemiology, economics and automated image identification such as handwritten zip codes \citep{Hastie2009}. Two major classes of machine learning methods are unsupervised and supervised learning. Unsupervised learning methods are used with unlabeled data where the underlying structure is estimated and are analogous to clustering and density estimation methods \citep{Kaufman1990}. Supervised learning methods are used with labeled data where the final output of data is known and the rules for going from input to output are inferred. These are analogous classification and regression models \citep{Breiman1984}. The application of the alternative approaches used in this study illustrates only a sampling of the various previously derived methods for clustering observations and fitting classification models. 

Geometric morphometric approaches to identifying differences in morphological variation between different classes, including cryptic species, mostly have used methods like linear discriminate analysis and canonical variates analysis \citep{Zelditch2004,Mitteroecker2011,Polly2007a,Polly2003,Gunduz2007,Gaubert2005b,Demandt2009,Sztencel-Jabonka2009,MitrovskiBogdanovic2013,Francoy2009}. These methods are comparatively straightforward ways of understanding the differences in morphology between classes given their similarity to familiar multivariate approaches like principal components analysis (PCA). They are benefit by producing results that can be easily visualized, which aids in the interpretation and presentation of data and results. Most previous morphometric studies did not assess which amongst a set of alternative classification hypotheses was optimal. For example, studies such as those of \citet{Caumul2005a} and \citet{Polly2007a} focused on comparing different aspects of morphology and their fidelity to a classification scheme instead of comparing the fidelity of one aspect of morphology to multiple classification schemes. In this context, the study of \citet{Cardini2009a}, is noteworthy because they compared morphological variation in marmots at the population, regional, and species level and determined the fidelity of shape to divisions at each of these levels.

Here, we used multiple machine learning methods, both unsupervised and supervised, in order to compare different classification hypotheses. These methods provide different and unique advantages for understanding how to classify taxa, and with what accuracy. While machine learning methods such as neural networks have been applied to studying shape variation \citep{Baylac2003,Dobigny2003,VandenBrink2011,MacLeod2007}, including in the context of automated taxon identification and classification of groups, the number of cases remains limited. In the current study, we not only consider pure classification accuracy but also use a statistic of classification strength that reflects the rate at which taxa are both accurately and inaccurately classified. 


\subsection{\textit{Emys marmorata}}
We analyzed the problem of whether there are distinct subspecies or cryptic species exist within the western pond turtle, \textit{Emys marmorata} (formerly \emph{Clemmys marmorata}; see \citealp{Feldman2002}). \textit{E. marmorata} is distributed from northern Washington State, USA to Baja California, Mexico. Traditionally, \textit{E. marmorata} was classified into two named subspecies: the northern \textit{E. marmorata marmorata}, the southern \textit{E. marmorata palida} \citep{Seeliger1945}. She also recognized a central Californian intergrade zone between these subspecies, and did not include the Baja California populations of \textit{E. marmorata} in these groups, implying the existence of a third distinct but unnamed subspecies. \textit{E. marmorata marmorata} is differentiated from \textit{E. marmorata palida} by the presence of a pair of triangular inguinal plates and darker neck markings. It should be noted that the triangular inguinal plates can sometimes be present in \textit{E. marmorata palida} though they are considerably smaller.

Previous work on morphological variation in \textit{E. marmorata} has focused primarily on differentiation between populations over a portion of the species' total range \citep{Lubcke2007,Germano2009,Germano2008,Bury2010}; comparatively few studies have included specimens from across the entire range \citep{Holland1992}. Most of these studies considered how local biotic and abiotic factors may contribute to differences in carapace length and found that size can vary greatly between different populations \citep{Lubcke2007,Germano2009,Germano2008}. There also has been interest in size-based sexual dimorphism in \textit{E. marmorata} \citep{Lubcke2007,Germano2009,Holland1992}, with males being on average larger than females based on total carapace length and other linear measurements. However, the quality of size as a classifier of sex can vary greatly between populations \citep{Holland1992}, which makes sense in light of the amount of between population size difference \citep{Lubcke2007,Germano2009}. However, the effect of sexual dimorphism on shape, \textit{sensu} \citet{Kendall1977a}, was not assessed \citep{Holland1992,Lubcke2007,Germano2008}.

Of particular importance in the context of cryptic diversity in \textit{E. marmorata} is the traditional morphometric analysis of carapace shape carried out by \citet{Holland1992}. He compared morphological differences between and among many populations of \textit{E. marmorata} in three different areas of the species' range, and studied the relative effect of distance versus barriers in fostering morphological differentiation in the species. \citet{Holland1992} concluded that geographic distance was a poor indicator of mophological differentiation, and instead geographic features such as breaks between different drainage basis are probably more important barriers to reproduction. Additionally, \citet{Holland1992} found that morphological differences were observable as the magnitude of barriers and distance increased, but the variation required many variables to adequately capture, implying only very subtle morphological differentiation between putatively distinct populations. \citet{Holland1992} concluded that \textit{E. marmorata} is best classified as three distinct species: a northern species, souther species, and a Columbia basin species. This classification is similar to that of \citet{Seeliger1945}, except elevated to the species level and without recognition of a distinct Baja species. 

More recently, the phylogeography of \textit{E. marmorata} and the possibility of cryptic diversity has been investigated using molecular data \citep{Spinks2005,Spinks2010,Spinks2014}. Based on mitochondrial DNA, \citet{Spinks2005} recognized four subclades within \textit{E. marmorata}, a northern clade, a San Joaquin Valley clade, a Santa Barbara clade, and a southern clade. Analyses with nuclear DNA \citep{Spinks2010} with single-nucleotide polymorphism (SNP) data suggest a primarily north--south division in \textit{E. marmorata}, although the dataset differs in the location of this break point. In these papers, Spinks and colleagues discussed the potential taxonomic implications of their results, with \citet{Spinks2014} going so far as to strongly advocate for the recognition of at least two species (\emph{E. marmorata} and \emph{E. pallida}), and a possible third based on populations in Baja California. However, they did not discuss in detail the morphological characters that would help to diagnose these species beyond those specified by \citet{Seeliger1945}. Given that these characters are somewhat variable within the proposed species, and that \citet{Holland1992} described shell shape variation that might be consistent with this taxonomy, we wondered whether a geometric morphometric analysis of shell shape would provide a more reliable way of diagnosing groups (whether species or subspecies) within \textit{E. marmorata}.

In this study, we attempt to estimate the best classification scheme of \textit{E. marmorata} based on variation in plastral (ventral shell) shape in order to determine whether this character is consistent with any of the past divisions based on other morphological features or molecular data. Because of unclear geographic boundaries between subgroups of \textit{E. marmorata}, we compare multiple hypotheses of morphologically-- and molecularly--based classification. We hypothesize that if morphological variation corresponds to class assignment, then it should be possible to determine the best classification hypothesis of \textit{E. marmorata} from amongst multiple candidate hypotheses. However, if morphological variation variation does not correspond to any classification hypothesis, then supervised learning model generalization performance will be poor and reflect how variation may not follow along with any of the candidate classification hypotheses.


\section{Materials and Methods}
\subsection{Specimens, sampling, morphometrics}
We collected landmark-based morphometric data from 354 adult \textit{E. marmorata} museum specimens. These specimens are a subset of those included in \citet{Angielczyk2007}, \citet{Angielczyk2011}, and \citet{Angielczyk2013a} and represents adult individuals. We chose to focus on adults because significant changes in plastron shape occur over the course of ontogeny in \textit{E. mamorata} and other emydines \citep{Angielczyk2013a}. \uppercase{Peter note -- how was something classified as an adult?}

We assigned a classification to each specimen for the different binning schemes based on geographic occurrence data recorded in museum collection archives. When precise latitude and longitude information was not available we estimated it from whatever locality information was present. Because the specimens sampled to obtain the genetic data used to define the subclades were not available for study, all specimen classifications were based solely on the geographic information, not explicit assignment in previous studies. Because the exact barriers between different biogeographic regions are unknown and unclear, we represented each hypothesis with two different schemes; we compared a total of six different schemes. The schemes differed based on where geographic boundaries were assigned. This changes how certain individuals were assigned two one of the groups within in hypothesis such as which of the three morphologically defined groups, which of the four mitochondrially defined groups, and so on. % How were adults chosen? Good to note how schemes differeed.

Following previous work on plastron variation \citep{Angielczyk2007,Angielczyk2011,Angielczyk2013a}, we used TpsDig 2.04 \citep{Rohlf2005} to digitize 19 landmarks (Fig. \ref{fig:plastra}). Seventeen of the landmarks are at the endpoints or intersection of the keratinous plastral scutes that cover the platron. Twelve of the landmarks were symmetrical across the axis of symmetry and, in order to prevent issues surrounding degrees of freedom and other similar concerns \citep{Klingenberg2002}, we reflected these landmarks across the axis of symmetry (i.e. midline) prior to analysis and used the average position of each symmetrical pair. In cases where damage or incompleteness prevented symmetric landmarks from being determined, we used only the single member of the pair. We conducted all subsequent analyses on the resulting ``half'' plastra. We superimposed the plastral landmark configurations using generalized Procrustes analysis \citep{Dryden1998a}, after which, we calculated the principal components (PC) of shape using the \texttt{shapes} package for R \citep{2013,Dryden2013}.


\subsection{Machine learning analyses}
\subsubsection{Unsupervised learning}
In order to preserve the relationship between all landmark configurations in shape space, we measured the dissimilarity between observations using Kendall's Riemannian shape distance or \(\rho\) \citep{Kendall1984a,Dryden1998a}. We chose this metric because shape space, or the set of all possible shape configurations following Procrustes superimposition, is a Riemannian manifold and thus non-Euclidean \citep{Dryden1998a}. \(\rho\) varies between 0 and \(\pi / 2\) when there is no reflection invariance, which should not be a concern in the case of the half plastral landmark configurations used in the study.

We divisively clustered the shape dissimilarity matrix using partitioning around mediods clustering (PAM), a method similar to \textit{k}-means clustering except that instead of minimizing the sum of squared Euclidean distances between observations and centroids, the sum of squared dissimilarities between observations and mediods is minimized \citep{Kaufman1990}. Because the optimal number of clusters of shape configurations in the study was unknown, being possibly three, four, or some other value, we estimated clustering solutions in which the number of clusters varied between one and eight. We compated clustering solutions using the gap statistic, which is a measure of goodness of clustering \citep{Tibshirani2001a}.

%The gap statistic is defined
%\[Gap_{n}(k) = E^{*}_{n}[\log(W_{k})] - \log(W_{k})\] 
%where \(W_{k}\) is
%\[W_{k} = \sum^{k}_{r = 1}{\frac{1}{2n_{r}} (\sum_{i,i' \in C_{r}} d_{ii'})}\].
%\(d_{ii'}\) is the dispersion of the clustering solution or the sum of the pairwise dissimilarities between observations in each cluster and their respective mediods (\(C\)) for all clusters \(r\). This value is averaged and compared to the expected dispersion (\(E^{*}_{n}\)) of a sample \(n\) from a reference distribution. In this case, the reference distribution was estimated from 500 resamples.

We conducted this analysis using the \texttt{cluster} package for R \citep{Maechler2013}.

\subsubsection{Supervised learning}
We used three different supervised learning, or classification, approaches: linear discriminate analysis, multinomial logistic regression, and random forests. Linear discriminate analysis, also known as canonical variate analysis, is commonly used in studies of geometric morphometric data \citep{Zelditch2004,Mitteroecker2011}. The other two methods, however, are not. In all cases, the optimal number of PCs used as predictors was chosen via maximum within-sample AUC value, explained below.

Linear discriminate analysis (LDA) attempts to find a linear combination of predictors that best model two or more classes. LDA is very similar to PCA except that instead of finding the linear combination of features that maximize the amount of explained variance in the data, LDA maximizes the differences between classes. The results of this analysis produces a transformation matrix by which the original features can be transformed to reflect the best discrimination between the classes. In this study, we applied LDA to the eigenscores from a subset of the total number of PCs, ranging from two to 6 in increasing order of complexity. In total, this produced nine different LDA scaling matrices. 

Multinomial logistic regression is an extension of logistic regression, where instead of a binary response there are three or more response classes \citep{Venables2002a}. Similar to the odds ratios calculated from the coefficients of a logistic regression, the relative risk of a classification can be determined from the coefficients of the model.

Random forest models are an extension of classification and regression trees (CART) \citep{Breiman1984,Breiman2001}. The goal of CARTs are to use a series of different features (i.e. predictors) to estimate the class of an observation. In top-down induction of decision trees for each member of a given set of predictor variables, attribute value tests are used to estimate the differences between classes. This process, called recursive partitioning, is then repeated on each subset. The recursion continues until the resulting observations all share the same class or no more meaningful partitions are possible. The resulting model is a tree structure by which observations are classified at each intersection via the estimated cutoff points from the attribute tests made during model fitting. % figure to help explain the ``tests''

In a random forest model, many CARTs are built from a random subsample of both the features and the observations (specimens). This process is then repeated many times and the parameters of the final model are chosen as the mode of estimates from the distribution of CARTs \citep{Breiman2001}. In addition to classifying the observations, this procedure allows for the features to be ranked in order of importance. This is a generally useful property for studies in which the goal is to describe and model the differences between classes and the relative importance of different predictors. 

In this analysis, we used 1000 subtrees to estimate the random forest model parameters. We estimated the best set of predictors necessary for each classification scheme was estimated using a recursive feature selection algorithm, and we chose the optimal number of PCs to include based on the AUC of the model. Following the backwards selection algorithm implemented in \texttt{caret} \citep{Kuhn2013}, the maximum number of features were included in the initial model, their importance ranked, and the AUC of the model calculated. The lowest ranked feature was then removed, and the AUC of the model recalculated. This was repeated until only one feature, remained. Because PCs were kept in order of importance and not in relation to the amount of variance each PC described, these means that the PCs are not included in the order of ascending eigenvalue.

In classification studies, such as this one, a common metric of performance is area under the receiver operating characteristic curve (AUC). AUC is an estimate of the relationship between the false positive and true positive rates, as opposed to just the true positive rate (accuracy). This relationship is especially useful in cases where misclassification needs to be minimized just as much as accurate classification, as in this study. AUC ranges between 0.5 and 1, with 0.5 indicating classification no better than random and 1 indicating perfect classification \citep{Hastie2009}.

The standard AUC calculation is defined for binary classifications, however in this application there are multiple categories. The alternative calculation that we used follows an all-against-one strategy where the individual AUC values for each class versus all others are averaged to produce a multiclass AUC \citep{Hand2001}. To estimate confidence intervals on the out-of-sample AUC values, we performed a nonparametric bootstrap in which the true and estimated classifications were resampled with replacement. This was done 1000 times.

The ultimate measure of model fit is accurately predicting the values of unobserved samples \citep{Hastie2009,Kuhn2013}. Within-sample performance is inherently biased upwards, so model evaluation requires overcoming this bias. With very large sample sizes, as in this study, part of the sample can be used as the ``training set'' and the remainder acts as the ``testing set.'' The former is used for fitting the model where as the later is used for measuring model performance, and this process is called model generalization. In this analysis, we used 75\% of samples as the training set while the remaining 25\% were used as the testing set.

It is common for some out of sample observations to be misclassified. This misclassification may be due to the model not accurately representing shape variance, systematic differences between the training and test sets, or systematic differences between the accurately and inaccurately classified samples. Testing and training sets are determined completely at random within each class and with respect to shape. Results were not effected by changes in testing or training set assignment.

To determine if there were systematic differences between the correctly and incorrectly classified samples, we used a permutation test to estimate if the dissimilarity between the correctly and incorrectly classified individuals were significantly different from random. The group labels were permuted 1000 times and the distance between the new centroids was calculated. The number of permutations less than the empirical difference divided by 1000 gives a \textit{p}-value for the test. Significant results indicate that correctly and incorrectly classified specimens are systematically different. This was done only for classes where there were 10 or more observations.



\section{Results}

\subsection{Unsupervised learning}
Comparison of gap statistic values from PAM clustering show that the optimal, minimal number of clusters is most likely one (Fig. \ref{fig:gap}). There is some ambiguity in choice because, although it is not statistically different from a solution with only one group, the solution with two groupings does have the greatest mean gap statistic. However, there is no geographical signal in the results of this clustering solution (Fig. \ref{fig:gap_map}). Because of this, we assert that this means that there is no means of naturally partitioning plastron shape into distinct subgroups with out reference to external information.
% PAM stuff
% figure

\subsection{Supervised learning}
% model selection/fit
AUC--based model selection revealed some important patterns of variation and congruence between the classification schemes and the actual data. Generally, the best performing models tended to include as many PCs as possible \ref{fig:sel}). Note that the best random forest models were determined via recursive feature selection, so PCs were not included in order of percent variance explained. That almost all LDA and multinomial logistic regression models were as complex as possible indicates that the differences between the different groups within each classification scheme are very small.

As part of fitting a random forest model, a ranking of variable importance also is determined. Interestingly, the order of variable importance is not the same as the order of the PCs (\ref{fig:var_imp}). This means that the variance describing the differences between the classes does not align with the major axes of variance (i.e. the PCs). This result would be the case if variation between classes was extremely fine grained and not a part of the principal form or function of the plastron, which makes sense given that the plastron is involved in both protection and hydrodynamics and not necessarily mate choice \citep{Germano2009,Holland1992,Lubcke2007,Rivera2008}. Moreover, this result is congruous with the results from the AUC--based model selection for the multinomial logistic regression and LDA models.

Observed AUC values for all of the optimal models are not exceptionally high (\ref{fig:sel}). In most cases the different proposed classification schemes are generally poor descriptors of the observed variation. It appears that the data set is overwhelmed by noise, making any accurate classifications difficult at best. This observation is cemented with the generalizations of the models to the testing data set (\ref{fig:gen_hist}).

% generalization
Mean AUC values for the model generalizations, in most cases, are approximately equal to the observed AUC values from the training data set (Table \ref{tab:comp}). The  cases in which the AUC from the  generalizations is less than the observed, indicate poor model fit and a poor classification scheme. AUC values from model generalization, or estimating testing data set membership, does not indicate a clear ``best'' classification scheme (Fig. \ref{fig:gen_hist}). Although the scheme with two species has the greatest AUC point estimate for each modeling approach, this scheme is not significantly greater than any other except in some limited cases (e.g. LDA, Table \ref{tab:gen_tests}). 
% misfits
Differences in mean shape between correctly and incorrectly classified observations from test set frequently were statistically significant, though there are exceptions. Again, this test was to determine if the mean shapes were statistically different or not. The frequency of these results, however, is important because it means that the different models are poor predictors of class membership. This may be because differences in plastron shape do not align with the any of the hypothesized classification schemes.


\section{Discussion}

The results of this study indicate that there is no clear grouping of \textit{E. marmorata} based on plastron shape.

The unsupervised learning results which indicate only a single group of observations being optimal is congruous with the results from the generalizations of the supervised learning models. The classification schemes used in the supervised learning models correspond, loosely, to unsupervised learning solutions with multiple groups. Because unsupervised learning solutions with multiple groups are poor descriptors of the observed variation, it is important to see this generally supported by the supervised learning results.

The results from fitting the various supervised learning models for each of the classification scheme generally shows that no one scheme is ``best.'' A possible explanation for this that the genetic divergence associated with (sub)speciation is either not based on plastron morphology or local selective pressures due to hydrological regime overwhelming any possible morphological divergence.

Both the low AUC values (\(< 0.9\)) and the significant difference between the correctly and incorrectly classified observations support the conclusion that none of the hypothesized classification schemes are good descriptors of the observed plastral variation.

INSERT MORE TEXT HERE.

\clearpage
\bibliographystyle{sysbio}
\bibliography{turtle,packages}

\clearpage

\begin{table}[ht]
  \centering
  \include{comp_tab}
  \caption{AUC values for the best model of each classification scheme for both the observed (training) data and the generalized (testing) data. Results from all three different supervised learning approaches are shown here. AUC values range between 0.5 and 1. }
  \label{tab:comp}
\end{table}

\begin{table}
  \include{rf_dif}
    %
  \include{mm_dif}
    %
  \include{ll_dif}
  \caption{Results of bootstrap comparisons between the scheme with the highest mean AUC value and all other schemes. An asterix indicates the best scheme. This was done for each of the three modeling techniques included in this study. Probabilities are the percent of comparisons that are greater than the observed difference in means.}
  \label{tab:gen_tests}
\end{table}

\begin{table}
  \begin{scriptsize}
    \include{rf_miss}

    \include{mm_miss}

    \include{ll_miss}
  \end{scriptsize}
  \caption{Results of comparisons between correctly and incorrectly classified observations from the testing data set. For each scheme, the classifications with at least 10 observations were tested. This was done for each of the three modeling techniques included in this study.}
  \label{tab:miss_tests}
\end{table}

\begin{figure}[ht]
  \centering
  \includegraphics[height = \textheight, width = \textwidth, keepaspectratio = true]{figure/plastra}
  \caption{Depiction of general plastral shape of \textit{E. marmorata} and position of the 19 landmark used in this study. Anterior is towards the top of the figure.}
  \label{fig:plastra}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[height = \textheight, width = \textwidth, keepaspectratio = true]{figure/gap_res}
  \caption{Results from PAM clustering of the Riemannian shape distance for 8 different number of clusters. Vertical lines are 1 standard deviation of the mean determined from 500 resamples.}
  \label{fig:gap}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[height = \textheight, width = \textwidth, keepaspectratio = true]{figure/gap_map}
  \caption{Comparison of geographic distribution of clustered observations from the 2 clustering PAM solution. Colour and shape correspond to each of the groups. There is clearly no geographic signal in the data.}
  \label{fig:gap_map}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[height = \textheight, width = \textwidth, keepaspectratio = true]{figure/sel_val}
  \caption{Graphical representation of the AUC values from model selection for multinomial logigistic regression and linear discriminate analysis, respecitively. AUC model selection is based on greatest AUC value. The horizontal axis corresponds to the cummulative number of axes included in the model of interest. A red dot corresponds to the AUC best model for that classification scheme.}
  \label{fig:sel}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[height = \textheight, width = \textwidth, keepaspectratio = true]{figure/var_imp}
  \caption{Variable importance from the random forest models for each of the six classification schemes. Importance is measured as the mean decrease in Gini Index, which is a measure of the strength by which that variable determines CART structure. Indices that are farther to the right indicate greater variable importance.}
  \label{fig:var_imp}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[height = \textheight, width = \textwidth, keepaspectratio = true]{figure/gen_res}
  \caption{Bootstrap distributions for genearlized AUC values for each of the classification schemes. Each row corresponds to a different modeling approach: LDA, LDA using best variables from random forest, multinomial logistic regression, and random forest. Each distribution corresponds to 1000 bootstrap replicates.}
  \label{fig:gen_hist}
\end{figure}

\end{document}
