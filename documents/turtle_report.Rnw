\documentclass{article}
\usepackage{amsmath, amsthm}
\usepackage{graphicx, microtype, parskip}
\usepackage{rotating, longtable, caption, subcaption}
\usepackage[sort&compress]{natbib}

\frenchspacing

<<message = FALSE, echo = FALSE, warning = FALSE>>=
require(knitr)
require(xtable)
require(reshape2)
require(ggplot2)
require(GGally)

opts_chunk$set(warning = FALSE, echo = FALSE, message = FALSE,
               highlight = TRUE, background = 'white')
@

<<results = 'hide'>>=
source('../src/support_functions.r')
source('../src/plotting_functions.r')
load('../src/turtle_analysis.RData')
@

<<>>=
map <- map_data('state')
st <- c('california', 'oregon', 'washington')
map <- map[map$region == st, ]

turtle.train <- lapply(turtle.train, 
                       function(x) {
                         x$long <- as.numeric(as.character(x$long)) * -1
                         x$lat <- as.numeric(as.character(x$lat))
                         x})
turtle.test <- lapply(turtle.test,
                      function(x) {
                        x$long <- as.numeric(as.character(x$long)) * -1
                        x$lat <- as.numeric(as.character(x$lat))
                        x})

labs <- Map(function(x, n) x[, n],
            x = turtle.train, n = names(turtle.train))
@

\title{Progress report: \textit{Emys marmorata} sub-species identification and classification}

\begin{document}
\maketitle


\section{Methods}
No-free lunch theorem. Try lots of things because we don't understand everything.

\subsection{Unsupervised}
Underlying structure in data?

\subsubsection{Gap-based clustering}
Comparison of gap statistic results for partitioning around medoids (PAM) divisive clustering. Confidence intervals are determined via bootstrap. The higher the gap statistic, the better the clustering result. Standard errors of the gap statistic were estimated from 100 resamples.


\subsubsection{Evidence Accumulation Clustering}
Choosing an optimal number of partitions is hard, which is why gap-based cluster selection was used above. An alternative method is to look at the co-occurrence frequency, that is how frequently any two samples occur in the same partition. Repeating this process over and over again creates the frequency, or ``vote'' for how the data set should be partitioned and  which specimens should be in the same cluster.

EAC was originally devised using \textit{k}-means clustering, but I've extended it to use PAM clustering instead. The hope is to determine underlying structure in the data given a wide enough partition range and a high enough number of iterations. Dissimilarity based EAC was performed using a range of 1 though \Sexpr{50 * 4} possible partitions and based on 10,000 iterations.

\subsection{Supervised}
How well does data conform to predetermined structure?

\begin{itemize}
  \item multinomial logistic regression (Fig. \ref{fig:multi-map})
  \item feed-forward neural networks (Fig. \ref{fig:nnet-map})
  \item random forests (Fig. \ref{fig:rf-map})
\end{itemize}


\section{Preliminary results}
\subsection{Unsupervised}
Comparison of gap statistic over a very wide range of plausible partitions indicates that as the number of partitions increase, there is a marginal increasing gap statistic until approximately \Sexpr{max(tmorph.km$clustering)} after which there is a marginal decrease in gap statistic (Fig. \ref{fig:gap}). It is notable that the standard errors around the gap statistic values are very large, and the marginal increases in gap statistic with an increased number of partitions may not be important. Additionally, all gap statiic values are within \Sexpr{max(tmorph.gap$Tab[, 3]) - min(tmorph.gap$Tab[, 3])} of each other meaning that there is little over all consensus for how many clusters are present when comparing gap statistics.

<<gap, eval = FALSE>>=
gg.gap <- gap.plot(tmorph.gap)
gg.gap
@
\begin{figure}[ht]
  \centering
  \includegraphics[width = \textwidth, keepaspectratio = true]{figure/gap}
  \caption{Gap statistic values for multiple PAM-based clustering configurations of the Riemmanian shape distances of the \textit{Emys marmorata} plastra. Higher values indicate greater clustering. Standard errors are estimated from 100 bootstrap resamples.}
  \label{fig:gap}
\end{figure}

Dissimilarity based EAC estimated approximately \Sexpr{max(tmorph.dac)} optimal partitions (Table \ref{tab:dac}).

<<dac-tab, results = 'asis'>>=
dac.tab <- xtable(table(tmorph.dac))
caption(dac.tab) <- 'Number of specimens assigned to optimal number of partitions as determined by dissimilarity based EAC. Each column corresponds to a different partition, with the number assigned directly below it.'
label(dac.tab) <- 'tab:dac'
print.xtable(dac.tab
             , table.placement = 'ht'
             )
@

<<gm, eval = FALSE>>=
gg.morph <- ggpairs(turtle.info, columns = c(1:3),
                    upper = 'blank')
fits <- procGPA(turtle.land.info)
ggland <- mshape.plot(fits, all.points = TRUE)
gg.morph <- putPlot(gg.morph, ggland, 1, 3)
gg.morph
@

\begin{figure}[ht]
  \centering
  \includegraphics[width = \textwidth]{figure/gm}
  \caption{Visualization of PCA of \textit{E. marmorata}. The lower triangle is the pairwise comparison of the first three pricipal components. The upper left corner is the comparison of landmark dispersion for all specimens compared to the mean shape in red.}
  \label{fig:gm}
\end{figure}


\subsection{Supervised}

I'm currently holding back on showing the tables because there are a lot of them. I'll show the confusion matrix for the multinomial logicistic regressions because they are easy (Tables \ref{tab:multi-conf-sh1}, \ref{tab:multi-conf-sh2}, \ref{tab:multi-conf-sh3}, \ref{tab:multi-conf-spinks})

<<multi-tab, results = 'asis'>>=
mt <- lapply(tm.conf, function(x) xtable(as.table(x)))

for(ii in seq(length(mt))) {
  caption(mt[[ii]]) <- paste0('Confusion matrix for ',
                              names(mt)[ii],
                              ' classification scheme. Rows correspond to predicted class. Columns correspond to reference class.')
  label(mt[[ii]]) <- paste0('tab:multi-conf-', names(mt)[ii])
}

for(jj in seq(length(mt))) {
  print.xtable(mt[[jj]]
               , table.placement = 'ht')
}


@

But I can show the comparison of the predictive accuracies (Fig. \ref{fig:resamp}).

<<resamp, eval = FALSE>>=
tmod.re.clean <- lapply(tmod.re, function(x) clean.resamp(x$values))
tmod.re.gg <- lapply(tmod.re.clean, resamp.plot)
tmod.re.gg
@

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \caption{}
    \includegraphics[width = \textwidth]{figure/resamp1}
    \label{fig:resamp1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \caption{}
    \includegraphics[width = \textwidth]{figure/resamp2}
    \label{fig:resamp2}
  \end{subfigure}\\

  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \caption{}
    \includegraphics[width = \textwidth]{figure/resamp3}
    \label{fig:resamp3}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \caption{}
    \includegraphics[width = \textwidth]{figure/resamp4}
    \label{fig:resamp4}
  \end{subfigure}
  \caption{Comparison of resampling distributions of training set accuracy and kappa statistics for the selected models of each classification scheme. \ref{fig:resamp1}: sh1 classification scheme. \ref{fig:resamp2}: sh2 classification scheme. \ref{fig:resamp3}: sh3 classification scheme. \ref{fig:resamp4}: spinks classification scheme.}
  \label{fig:resamp}
\end{figure}


In the interest of space/time, I'm only going to display results from one of the selected models from each method for each classification scheme.

<<multi-map, eval = FALSE>>=
tm.map <- mapply(class.map
                 , train = turtle.train
                 , test = turtle.test
                 , label = labs
                 , pred = tm.class
                 , MoreArgs = list(map = map)
                 , SIMPLIFY = FALSE)
tm.map
@

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \caption{}
    \includegraphics[width = \textwidth]{figure/multi-map1}
    \label{fig:multi-map1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \caption{}
    \includegraphics[width = \textwidth]{figure/multi-map2}
    \label{fig:multi-map2}
  \end{subfigure}\\

  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \caption{}
    \includegraphics[width = \textwidth]{figure/multi-map3}
    \label{fig:multi-map3}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \caption{}
    \includegraphics[width = \textwidth]{figure/multi-map4}
    \label{fig:multi-map4}
  \end{subfigure}
  \caption{Geographic position of all turtles sampled. Both training and testing observations are plotted. Training set observations are circles while testing observations are larger squares. Testing set observations are classified based on a multinomial logistic regression model. \ref{fig:multi-map1}: sh1 classification scheme. \ref{fig:multi-map2}: sh2 classification scheme. \ref{fig:multi-map3}: sh3 classification scheme. \ref{fig:multi-map4}: spinks classification scheme.}
  \label{fig:multi-map}
\end{figure}

<<nnet-map, eval = FALSE>>=
tnn.class.tiny <- lapply(tnn.class, function(x) x[[1]])
tnn.map <- mapply(class.map
                 , train = turtle.train
                 , test = turtle.test
                 , label = labs
                 , pred = tnn.class.tiny
                 , MoreArgs = list(map = map)
                 , SIMPLIFY = FALSE)
tnn.map
@

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \caption{}
    \includegraphics[width = \textwidth]{figure/nnet-map1}
    \label{fig:nnet-map1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \caption{}
    \includegraphics[width = \textwidth]{figure/nnet-map2}
    \label{fig:nnet-map2}
  \end{subfigure}\\

  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \caption{}
    \includegraphics[width = \textwidth]{figure/nnet-map3}
    \label{fig:nnet-map3}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \caption{}
    \includegraphics[width = \textwidth]{figure/nnet-map4}
    \label{fig:nnet-map4}
  \end{subfigure}
  \caption{Geographic position of all turtles sampled. Both training and testing observations are plotted. Training set observations are circles while testing observations are larger squares. Testing set observations are classified based on a feed-forward single layer neural network model. \ref{fig:nnet-map1}: sh1 classification scheme. \ref{fig:nnet-map2}: sh2 classification scheme. \ref{fig:nnet-map3}: sh3 classification scheme. \ref{fig:nnet-map4}: spinks classification scheme.}
  \label{fig:nnet-map}
\end{figure}


<<rf-map, eval = FALSE>>=
trf.class.tiny <- lapply(trf.class, function(x) x[[1]])
trf.map <- mapply(class.map
                 , train = turtle.train
                 , test = turtle.test
                 , label = labs
                 , pred = trf.class.tiny
                 , MoreArgs = list(map = map)
                 , SIMPLIFY = FALSE)
trf.map
@

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \caption{}
    \includegraphics[width = \textwidth]{figure/rf-map1}
    \label{fig:rf-map1}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \caption{}
    \includegraphics[width = \textwidth]{figure/rf-map2}
    \label{fig:rf-map2}
  \end{subfigure}\\

  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \caption{}
    \includegraphics[width = \textwidth]{figure/rf-map3}
    \label{fig:rf-map3}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \centering
    \caption{}
    \includegraphics[width = \textwidth]{figure/rf-map4}
    \label{fig:rf-map4}
  \end{subfigure}
  \caption{Geographic position of all turtles sampled. Both training and testing observations are plotted. Training set observations are circles while testing observations are larger squares. Testing set observations are classified based on a random forest model. \ref{fig:rf-map1}: sh1 classification scheme. \ref{fig:rf-map2}: sh2 classification scheme. \ref{fig:rf-map3}: sh3 classification scheme. \ref{fig:rf-map4}: spinks classification scheme.}
  \label{fig:rf-map}
\end{figure}


\section{Future}
\begin{enumerate}
  \item remove juvelines
  \item look into multinomial logistic mixed-effects models
  \item other unsupervised methods, though that might have to wait for a follow up paper
    \begin{itemize}
      \item bayesian nonparametrics for categorical data
    \end{itemize}
\end{enumerate}


\section{Miscellaneous affairs}
\subsection{Evolution 2013}
How cryptic is cryptic diversity? Machine learning approaches to fine scale variation in morphology of \textit{Emys marmorata}.


\subsection{Grants}
\subsubsection{Hinds Fund}
Resubmit previous application with the results from this study in the prelim-results section?

Get network for one turtle? This should take long, just requires time.

\subsubsection{Paleontological Society}
Why not? Need to wait till next year. Should probably join PaleoSoc anyway.


\end{document}
